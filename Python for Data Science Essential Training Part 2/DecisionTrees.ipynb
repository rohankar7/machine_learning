{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751396e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "\n",
    "# A decision tree is a decision-support tool that models decisions in order to predict probable outcomes of those decisions.\n",
    "\n",
    "# Types of Nodes in a Decision Tres\n",
    "# Root Node\n",
    "# Decision Nodes\n",
    "# Leaf Nodes\n",
    "\n",
    "# Decision Tree Algorithms\n",
    "# A class of suprevised machine learning methods that are useful for making predictions from nonlinear data\n",
    "\n",
    "# Decision tree algorithms are an appropriate fit for:\n",
    "# Continuous input and/or output variables\n",
    "# Categorical input and/or output variables\n",
    "\n",
    "# Two types of decision trees\n",
    "# 1) Categorical variable decision tree\n",
    "# 2) Continuous variable decision tree\n",
    "\n",
    "# Benefits of Decision Trees\n",
    "# Decision trees are nonlinear models\n",
    "# Decision trees are very easy to interpret\n",
    "# Trees can be graphically represented with ease, helping in interpretation\n",
    "# Decision Trees require less data preparation\n",
    "# Takes care of categorical variableswith one-hot encoding\n",
    "\n",
    "# Assumptions of Decision Trees\n",
    "# Root node = Entire training set\n",
    "# Predictive features are either categorical, or (if continuous) they're binned prior to model deployment\n",
    "# Rows in the dataset have a recursive distribution based on the values of attributes\n",
    "\n",
    "# How decision tree algorithms work\n",
    "# 1) Deploy recursive binary splitting to stratify or segment the predictor space into a number of simple, non-overlapping regions.\n",
    "# 2) Make a prediction for a given observation by using the mean or the mode of the training observations in the region to which it belongs\n",
    "# 3) Use the set of splitting rules to summarize the tree\n",
    "\n",
    "# Recursive Binary Splitting\n",
    "\n",
    "# It is a process that is used to segment a predictor space into regions in order to create a binary decision tree.\n",
    "# Recursive binary splitting stops when the user-defined criteria is met.\n",
    "\n",
    "# How it works:\n",
    "# At every stage we split the region into two, and we do this by the following criteria:\n",
    "# In Regression trees: we use the SSE (Sum of Square Error) to calculate the loss function, thus identifying the best split\n",
    "# In Classification trees: use the Gini index to calculate the loss function, thus identifying the best split\n",
    "\n",
    "# Characteristics of Recursive Binary Splitting\n",
    "# Top-down\n",
    "# Greedy\n",
    "\n",
    "\n",
    "# Disadvantages\n",
    "# Very non-robust\n",
    "# Sensitive to training data\n",
    "# Globally optimum tree not guaranteed\n",
    "\n",
    "# Appropriateness of using decision trees\n",
    "# 1) Target is a continuous variable\n",
    "# 2) Linear relationship between features and target\n",
    "\n",
    "# Note: Output variables from terminal nodes represent the mean response\n",
    "# Values of new data points will be predicted from that mean\n",
    "\n",
    "# Tree pruning\n",
    "# Tree pruning is the process that's used to overcome model overfitting by removing subnodes of a decision tree (that is, replacing a whole subtree by a leaf node).\n",
    "# Two types of tree pruning\n",
    "# 1) Hold-out test: fastest and simple pruning method\n",
    "# 2) Cost-complexity pruning: weakest link pruning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
