{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c7fa7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 08:08:43.191112: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-17 08:08:43.361187: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-17 08:08:43.361281: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-17 08:08:43.366468: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-17 08:08:43.392863: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-17 08:08:43.394874: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-17 08:08:47.043969: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#Deep Learning Foundations: Natural Language Processing with Tensorflow\n",
    "\n",
    "#import the libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e52ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define training sentences\n",
    "\n",
    "train_sentences = [\n",
    "    'It is a sunny day',\n",
    "    'It is a cloudy day',\n",
    "    'Will it rain today?'\n",
    "    #add a new sentence here\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0d2b045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'it': 1, 'is': 2, 'a': 3, 'day': 4, 'sunny': 5, 'cloudy': 6, 'will': 7, 'rain': 8, 'today': 9}\n"
     ]
    }
   ],
   "source": [
    "#Set up the tokenizer\n",
    "\n",
    "#instantiate the tokenizer\n",
    "tokenizer = Tokenizer(num_words=100)\n",
    "\n",
    "#train the tokenizer on training sentences\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "#store the word index for the words in the sentence\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42b6f898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating sequences of tokens\n",
    "\n",
    "#create sequences using tokenizer\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6342a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word index -->{'it': 1, 'is': 2, 'a': 3, 'day': 4, 'sunny': 5, 'cloudy': 6, 'will': 7, 'rain': 8, 'today': 9}\n",
      "Sequences of words -->[[1, 2, 3, 5, 4], [1, 2, 3, 6, 4], [7, 1, 8, 9]]\n"
     ]
    }
   ],
   "source": [
    "#print word index dictionary and sequences\n",
    "\n",
    "print(f'Word index -->{word_index}')\n",
    "print(f'Sequences of words -->{sequences}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83ce1a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a sunny day\n",
      "[1, 2, 3, 5, 4]\n"
     ]
    }
   ],
   "source": [
    "#print sample sentence and sequence\n",
    "print(train_sentences[0])\n",
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12f7ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize new data using the same tokenizer\n",
    "\n",
    "new_sentences = [\n",
    "    'Will it be raining today?',\n",
    "    'It is a pleasant day.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6da7a19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Will it be raining today?', 'It is a pleasant day.']\n",
      "[[7, 1, 9], [1, 2, 3, 4]]\n"
     ]
    }
   ],
   "source": [
    "new_sequences = tokenizer.texts_to_sequences(new_sentences)\n",
    "\n",
    "print(new_sentences)\n",
    "print(new_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c93fdd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing newly encountered words with oov tokens\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100, oov_token = '<oov>')\n",
    "\n",
    "#train the new tokenizer on training sentences\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "#sore word index for the words in the sentence\n",
    "\n",
    "word_index = tokenizer.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9a4589c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<oov>': 1, 'it': 2, 'is': 3, 'a': 4, 'day': 5, 'sunny': 6, 'cloudy': 7, 'will': 8, 'rain': 9, 'today': 10}\n",
      "[[8, 2, 1, 1, 10], [2, 3, 4, 1, 5]]\n"
     ]
    }
   ],
   "source": [
    "#create sequences of the new sentences\n",
    "\n",
    "new_sequences = tokenizer.texts_to_sequences(new_sentences)\n",
    "print(word_index)\n",
    "print(new_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa345727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Will it be raining today?', 'It is a pleasant day.']\n"
     ]
    }
   ],
   "source": [
    "print(new_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52ab0a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding the sequences\n",
    "\n",
    "#import the apis\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "441bcff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the training sentences\n",
    "\n",
    "train_sentences = [\n",
    "    'It will rain',\n",
    "    'The weather is cloudy',\n",
    "    'Will it be raining today?',\n",
    "    'It is a super hot day!',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddd9aab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the tokenizer\n",
    "\n",
    "#set up the tokenizer again with oov token\n",
    "tokenizer = Tokenizer(num_words=100, oov_token='<oov>')\n",
    "\n",
    "#train the tokenizer on training sentences\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "#store word index for words in the sentence\n",
    "\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f76667f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create sequences\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "608928ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pad Sequences\n",
    "\n",
    "padded_seqs = pad_sequences(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6032a096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<oov>': 1, 'it': 2, 'will': 3, 'is': 4, 'rain': 5, 'the': 6, 'weather': 7, 'cloudy': 8, 'be': 9, 'raining': 10, 'today': 11, 'a': 12, 'super': 13, 'hot': 14, 'day': 15}\n",
      "['It will rain', 'The weather is cloudy', 'Will it be raining today?', 'It is a super hot day!']\n",
      "[[2, 3, 5], [6, 7, 4, 8], [3, 2, 9, 10, 11], [2, 4, 12, 13, 14, 15]]\n",
      "[[ 0  0  0  2  3  5]\n",
      " [ 0  0  6  7  4  8]\n",
      " [ 0  3  2  9 10 11]\n",
      " [ 2  4 12 13 14 15]]\n"
     ]
    }
   ],
   "source": [
    "print(word_index)\n",
    "print(train_sentences)\n",
    "print(sequences)\n",
    "print(padded_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5cfba33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Customizing the padded sequence with parameters\n",
    "\n",
    "padded_seqs = pad_sequences(sequences,\n",
    "                           padding = 'post',\n",
    "                           maxlen=5,\n",
    "                           truncating='post',\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dda59474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  3  5  0  0]\n",
      " [ 6  7  4  8  0]\n",
      " [ 3  2  9 10 11]\n",
      " [ 2  4 12 13 14]]\n"
     ]
    }
   ],
   "source": [
    "print(padded_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e3ab04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment Analysis - Tokenizing news headlines for data preparation\n",
    "\n",
    "#Download news headlines\n",
    "\n",
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/wdd-2-node.appspot.com/x1.json \\\n",
    "    -o /tmp/headlines.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "57934dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>headline</th>\n",
       "      <th>article_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
       "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>dem rep. totally nails why congress is falling...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>inclement weather prevents liar from getting t...</td>\n",
       "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>mother comes pretty close to using word 'strea...</td>\n",
       "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_sarcastic                                           headline  \\\n",
       "0             1  thirtysomething scientists unveil doomsday clo...   \n",
       "1             0  dem rep. totally nails why congress is falling...   \n",
       "2             0  eat your veggies: 9 deliciously different recipes   \n",
       "3             1  inclement weather prevents liar from getting t...   \n",
       "4             1  mother comes pretty close to using word 'strea...   \n",
       "\n",
       "                                        article_link  \n",
       "0  https://www.theonion.com/thirtysomething-scien...  \n",
       "1  https://www.huffingtonpost.com/entry/donna-edw...  \n",
       "2  https://www.huffingtonpost.com/entry/eat-your-...  \n",
       "3  https://local.theonion.com/inclement-weather-p...  \n",
       "4  https://www.theonion.com/mother-comes-pretty-c...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the data using the pandas library\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_json(\"./x1.json\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ea75e52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segregate the headlines\n",
    "\n",
    "headlines = list(data['headline'])\n",
    "labels = list(data['is_sarcastic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8e74bcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the apis\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a36d7c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the data\n",
    "\n",
    "#instantiate the tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(oov_token='<oov')\n",
    "\n",
    "#train the tokenizer on training sentences\n",
    "\n",
    "tokenizer.fit_on_texts(headlines)\n",
    "\n",
    "#store word index for words in the sentences\n",
    "\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5ab83247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16004   355  3167  7474  2644     3   661  1119     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "#Create sequences\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(headlines)\n",
    "\n",
    "#create padded sequences\n",
    "padded_seqs = pad_sequences(sequences, padding=\"post\")\n",
    "\n",
    "#print the sequences sample\n",
    "print(padded_seqs[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
